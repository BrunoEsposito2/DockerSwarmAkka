akka {
  home = ""

  version = "2.8.6"
  stdout-loglevel = WARNING

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "DEBUG"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  logger-startup-timeout = 5s

  log-config-on-start = off

  log-dead-letters = on
  log-dead-letters-during-shutdown = on
  log-dead-letters-suspend-duration = 5 minutes

  loggers-dispatcher = akka.actor.default-dispatcher

  daemonic = off

  jvm-exit-on-fatal-error = on
  jvm-shutdown-hooks = on
  fail-mixed-versions = on

  coordinated-shutdown {
    default-phase-timeout = 5s
    terminate-actor-system = on
    exit-jvm = off
    exit-code = 0
    run-by-jvm-shutdown-hook = on
    run-by-actor-system-terminate = on
    reason-overrides {
        "akka.actor.CoordinatedShutdown$ClusterDowningReason$" {
            exit-code = -1
        }
        "akka.actor.CoordinatedShutdown$ClusterJoinUnsuccessfulReason$" {
            exit-code = -1
        }
    }

    phases {
      # The first pre-defined phase that applications can add tasks to.
      # Note that more phases can be added in the application's
      # configuration by overriding this phase with an additional
      # depends-on.
      before-service-unbind {
      }

      # Stop accepting new incoming connections.
      # This is where you can register tasks that makes a server stop accepting new connections. Already
      # established connections should be allowed to continue and complete if possible.
      service-unbind {
        depends-on = [before-service-unbind]
      }

      # Wait for requests that are in progress to be completed.
      # This is where you register tasks that will wait for already established connections to complete, potentially
      # also first telling them that it is time to close down.
      service-requests-done {
        depends-on = [service-unbind]
      }

      # Final shutdown of service endpoints.
      # This is where you would add tasks that forcefully kill connections that are still around.
      service-stop {
        depends-on = [service-requests-done]
      }

      # Phase for custom application tasks that are to be run
      # after service shutdown and before cluster shutdown.
      before-cluster-shutdown {
        depends-on = [service-stop]
      }

      # Graceful shutdown of the Cluster Sharding regions.
      # This phase is not meant for users to add tasks to.
      cluster-sharding-shutdown-region {
        timeout = 10 s
        depends-on = [before-cluster-shutdown]
      }

      # Emit the leave command for the node that is shutting down.
      # This phase is not meant for users to add tasks to.
      cluster-leave {
        depends-on = [cluster-sharding-shutdown-region]
      }

      # Shutdown cluster singletons
      # This is done as late as possible to allow the shard region shutdown triggered in
      # the "cluster-sharding-shutdown-region" phase to complete before the shard coordinator is shut down.
      # This phase is not meant for users to add tasks to.
      cluster-exiting {
        timeout = 10 s
        depends-on = [cluster-leave]
      }

      # Wait until exiting has been completed
      # This phase is not meant for users to add tasks to.
      cluster-exiting-done {
        depends-on = [cluster-exiting]
      }

      # Shutdown the cluster extension
      # This phase is not meant for users to add tasks to.
      cluster-shutdown {
        depends-on = [cluster-exiting-done]
      }

      # Phase for custom application tasks that are to be run
      # after cluster shutdown and before ActorSystem termination.
      before-actor-system-terminate {
        depends-on = [cluster-shutdown]
      }

      # Last phase. See terminate-actor-system and exit-jvm above.
      # Don't add phases that depends on this phase because the
      # dispatcher and scheduler of the ActorSystem have been shutdown.
      # This phase is not meant for users to add tasks to.
      actor-system-terminate {
        timeout = 10 s
        depends-on = [before-actor-system-terminate]
      }
    }
  }

  actor {
    default-mailbox {
      stash-capacity = 1000
      mailbox-type = "akka.dispatch.SingleConsumerOnlyUnboundedMailbox"
    }

    default-blocking-io-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      throughput = 1

      thread-pool-executor {
              # Keep alive time for threads
              keep-alive-time = 60s

              # Define a fixed thread pool size with this property. The corePoolSize
              # and the maximumPoolSize of the ThreadPoolExecutor will be set to this
              # value, if it is defined. Then the other pool-size properties will not
              # be used.
              #
              # Valid values are: `off` or a positive integer.
              fixed-pool-size = off

              # Min number of threads to cap factor-based corePoolSize number to
              core-pool-size-min = 8

              # The core-pool-size-factor is used to determine corePoolSize of the
              # ThreadPoolExecutor using the following formula:
              # ceil(available processors * factor).
              # Resulting size is then bounded by the core-pool-size-min and
              # core-pool-size-max values.
              core-pool-size-factor = 3.0

              # Max number of threads to cap factor-based corePoolSize number to
              core-pool-size-max = 64

              # Minimum number of threads to cap factor-based maximumPoolSize number to
              max-pool-size-min = 8

              # The max-pool-size-factor is used to determine maximumPoolSize of the
              # ThreadPoolExecutor using the following formula:
              # ceil(available processors * factor)
              # The maximumPoolSize will not be less than corePoolSize.
              # It is only used if using a bounded task queue.
              max-pool-size-factor  = 3.0

              # Max number of threads to cap factor-based maximumPoolSize number to
              max-pool-size-max = 64

              # Specifies the bounded capacity of the task queue (< 1 == unbounded)
              task-queue-size = -1

              # Specifies which type of task queue will be used, can be "array" or
              # "linked" (default)
              task-queue-type = "linked"

              # Allow core threads to time out
              allow-core-timeout = on
            }
    }

    mailbox {
        requirements {
            "akka.dispatch.UnboundedMessageQueueSemantics" =
              akka.actor.mailbox.unbounded-queue-based
            "akka.dispatch.BoundedMessageQueueSemantics" =
              akka.actor.mailbox.bounded-queue-based
            "akka.dispatch.DequeBasedMessageQueueSemantics" =
              akka.actor.mailbox.unbounded-deque-based
            "akka.dispatch.UnboundedDequeBasedMessageQueueSemantics" =
              akka.actor.mailbox.unbounded-deque-based
            "akka.dispatch.BoundedDequeBasedMessageQueueSemantics" =
              akka.actor.mailbox.bounded-deque-based
            "akka.dispatch.MultipleConsumerSemantics" =
              akka.actor.mailbox.unbounded-queue-based
            "akka.dispatch.ControlAwareMessageQueueSemantics" =
              akka.actor.mailbox.unbounded-control-aware-queue-based
            "akka.dispatch.UnboundedControlAwareMessageQueueSemantics" =
              akka.actor.mailbox.unbounded-control-aware-queue-based
            "akka.dispatch.BoundedControlAwareMessageQueueSemantics" =
              akka.actor.mailbox.bounded-control-aware-queue-based
            "akka.event.LoggerMessageQueueSemantics" =
              akka.actor.mailbox.logger-queue
          }

          unbounded-queue-based {
            # FQCN of the MailboxType, The Class of the FQCN must have a public
            # constructor with (akka.actor.ActorSystem.Settings,
            # com.typesafe.config.Config) parameters.
            mailbox-type = "akka.dispatch.UnboundedMailbox"
          }

          bounded-queue-based {
            # FQCN of the MailboxType, The Class of the FQCN must have a public
            # constructor with (akka.actor.ActorSystem.Settings,
            # com.typesafe.config.Config) parameters.
            mailbox-type = "akka.dispatch.BoundedMailbox"
          }

          unbounded-deque-based {
            # FQCN of the MailboxType, The Class of the FQCN must have a public
            # constructor with (akka.actor.ActorSystem.Settings,
            # com.typesafe.config.Config) parameters.
            mailbox-type = "akka.dispatch.UnboundedDequeBasedMailbox"
          }

          bounded-deque-based {
            # FQCN of the MailboxType, The Class of the FQCN must have a public
            # constructor with (akka.actor.ActorSystem.Settings,
            # com.typesafe.config.Config) parameters.
            mailbox-type = "akka.dispatch.BoundedDequeBasedMailbox"
          }

          unbounded-control-aware-queue-based {
            # FQCN of the MailboxType, The Class of the FQCN must have a public
            # constructor with (akka.actor.ActorSystem.Settings,
            # com.typesafe.config.Config) parameters.
            mailbox-type = "akka.dispatch.UnboundedControlAwareMailbox"
          }

          bounded-control-aware-queue-based {
            # FQCN of the MailboxType, The Class of the FQCN must have a public
            # constructor with (akka.actor.ActorSystem.Settings,
            # com.typesafe.config.Config) parameters.
            mailbox-type = "akka.dispatch.BoundedControlAwareMailbox"
          }

          # The LoggerMailbox will drain all messages in the mailbox
          # when the system is shutdown and deliver them to the StandardOutLogger.
          # Do not change this unless you know what you are doing.
          logger-queue {
            mailbox-type = "akka.event.LoggerMailboxType"
          }
        logger-mailbox {
          mailbox-type = "akka.dispatch.SingleConsumerOnlyUnboundedMailbox"
        }
    }

    default-dispatcher {
      type = "Dispatcher"
      executor = "fork-join-executor"
      throughput = 10
      throughput-deadline-time = 0ms
      shutdown-timeout = 1s

        fork-join-executor {
            # Min number of threads to cap factor-based parallelism number to
            parallelism-min = 8

            # The parallelism factor is used to determine thread pool size using the
            # following formula: ceil(available processors * factor). Resulting size
            # is then bounded by the parallelism-min and parallelism-max values.
            parallelism-factor = 1.0

            # Max number of threads to cap factor-based parallelism number to
            parallelism-max = 64

            # Setting to "FIFO" to use queue like peeking mode which "poll" or "LIFO" to use stack
            # like peeking mode which "pop".
            task-peeking-mode = "FIFO"
          }


      # For BalancingDispatcher: If the balancing dispatcher should attempt to
      # schedule idle actors using the same dispatcher when a message comes in,
      # and the dispatchers ExecutorService is not fully busy already.
      attempt-teamwork = on

      # If this dispatcher requires a specific type of mailbox, specify the
      # fully-qualified class name here; the actually created mailbox will
      # be a subtype of this type. The empty string signifies no requirement.
      mailbox-requirement = ""
    }

    router {
      type-mapping {
        round-robin-pool = "akka.routing.RoundRobinPool"
        round-robin-group = "akka.routing.RoundRobinGroup"
        random-pool = "akka.routing.RandomPool"
        random-group = "akka.routing.RandomGroup"
        balancing-pool = "akka.routing.BalancingPool"
        smallest-mailbox-pool = "akka.routing.SmallestMailboxPool"
        broadcast-pool = "akka.routing.BroadcastPool"
        broadcast-group = "akka.routing.BroadcastGroup"
        scatter-gather-pool = "akka.routing.ScatterGatherFirstCompletedPool"
        scatter-gather-group = "akka.routing.ScatterGatherFirstCompletedGroup"
        consistent-hashing-pool = "akka.routing.ConsistentHashingPool"
        consistent-hashing-group = "akka.routing.ConsistentHashingGroup"
      }
    }

    debug {
        receive = on // This enables logging of received messages
        lifecycle = on // This enables logging of actor lifecycle events
        autoreceive = off
        fsm = off
        event-stream = off
        unhandled = on
        router-misconfiguration = on
    }

    deployment {
        default {

            remote = ""

            dispatcher = my-default-dispatcher
            virtual-nodes-factor = 1

            cluster {
              enabled = on
              allow-local = on
              allow-local-routees = on
              use-role = ""
              use-roles = []
              max-nr-of-instances-per-node = 1
            }

            target {
              # A list of hostnames and ports for instantiating the children of a
              # router
              #   The format should be on "akka://sys@host:port", where:
              #    - sys is the remote actor system name
              #    - hostname can be either hostname or IP address the remote actor
              #      should connect to
              #    - port should be the port for the remote server on the other node
              # The number of actor instances to be spawned is still taken from the
              # nr-of-instances setting as for local routers; the instances will be
              # distributed round-robin among the given nodes.
              nodes = []

            }

            optimal-size-exploring-resizer {
              enabled = off

              # The fewest number of routees the router should ever have.
              lower-bound = 1

              # The most number of routees the router should ever have.
              # Must be greater than or equal to lower-bound.
              upper-bound = 10

              # probability of doing a ramping down when all routees are busy
              # during exploration.
              chance-of-ramping-down-when-full = 0.2

              # Interval between each resize attempt
              action-interval = 5s

              # If the routees have not been fully utilized (i.e. all routees busy)
              # for such length, the resizer will downsize the pool.
              downsize-after-underutilized-for = 72h

              # Duration exploration, the ratio between the largest step size and
              # current pool size. E.g. if the current pool size is 50, and the
              # explore-step-size is 0.1, the maximum pool size change during
              # exploration will be +- 5
              explore-step-size = 0.1

              # Probability of doing an exploration v.s. optimization.
              chance-of-exploration = 0.4

              # When downsizing after a long streak of under-utilization, the resizer
              # will downsize the pool to the highest utilization multiplied by a
              # a downsize ratio. This downsize ratio determines the new pools size
              # in comparison to the highest utilization.
              # E.g. if the highest utilization is 10, and the down size ratio
              # is 0.8, the pool will be downsized to 8
              downsize-ratio = 0.8

              # When optimizing, the resizer only considers the sizes adjacent to the
              # current size. This number indicates how many adjacent sizes to consider.
              optimization-range = 16

              # The weight of the latest metric over old metrics when collecting
              # performance metrics.
              # E.g. if the last processing speed is 10 millis per message at pool
              # size 5, and if the new processing speed collected is 6 millis per
              # message at pool size 5. Given a weight of 0.3, the metrics
              # representing pool size 5 will be 6 * 0.3 + 10 * 0.7, i.e. 8.8 millis
              # Obviously, this number should be between 0 and 1.
              weight-of-latest-metric = 0.5
            }

            resizer {
              enabled = off

              # The fewest number of routees the router should ever have.
              lower-bound = 1

              # The most number of routees the router should ever have.
              # Must be greater than or equal to lower-bound.
              upper-bound = 10

              # Threshold used to evaluate if a routee is considered to be busy
              # (under pressure). Implementation depends on this value (default is 1).
              # 0:   number of routees currently processing a message.
              # 1:   number of routees currently processing a message has
              #      some messages in mailbox.
              # > 1: number of routees with at least the configured pressure-threshold
              #      messages in their mailbox. Note that estimating mailbox size of
              #      default UnboundedMailbox is O(N) operation.
              pressure-threshold = 1

              # Percentage to increase capacity whenever all routees are busy.
              # For example, 0.2 would increase 20% (rounded up), i.e. if current
              # capacity is 6 it will request an increase of 2 more routees.
              rampup-rate = 0.2

              # Minimum fraction of busy routees before backing off.
              # For example, if this is 0.3, then we'll remove some routees only when
              # less than 30% of routees are busy, i.e. if current capacity is 10 and
              # 3 are busy then the capacity is unchanged, but if 2 or less are busy
              # the capacity is decreased.
              # Use 0.0 or negative to avoid removal of routees.
              backoff-threshold = 0.3

              # Fraction of routees to be removed when the resizer reaches the
              # backoffThreshold.
              # For example, 0.1 would decrease 10% (rounded up), i.e. if current
              # capacity is 9 it will request an decrease of 1 routee.
              backoff-rate = 0.1

              # Number of messages between resize operation.
              # Use 1 to resize before each message.
              messages-per-resize = 10
            }


        }

        "/SD-DNS/async-dns" {
            mailbox = "unbounded"
            router = "round-robin-pool"
            nr-of-instances = 1
        }
    }

    no-serialization-verification-needed-class-prefix = ["akka."]
    serialize-creators = off // Usually keep this off
    serialize-messages = off  // Usually keep this off
    unstarted-push-timeout = 10s
    creation-timeout = 30s
    guardian-supervisor-strategy = "akka.actor.OneForOneStrategy"

    serializers {
      akka-data-replication = "akka.cluster.ddata.protobuf.ReplicatorMessageSerializer"
      akka-replicated-data = "akka.cluster.ddata.protobuf.ReplicatedDataSerializer"
    }

    serialization-bindings {
      "akka.cluster.ddata.Replicator$ReplicatorMessage" = akka-data-replication
      "akka.cluster.ddata.ReplicatedDataSerialization" = akka-replicated-data
    }

    serialization-identifiers {
      "akka.cluster.ddata.protobuf.ReplicatedDataSerializer" = 11
      "akka.cluster.ddata.protobuf.ReplicatorMessageSerializer" = 12
    }

    internal-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      throughput = 5
    }
  }

  cluster.distributed-data {
    name = ddataReplicator
    role = ""
    gossip-interval = 2 s
    notify-subscribers-interval = 500 ms
    log-data-size-exceeding = 10 KiB
    max-delta-elements = 500
    use-dispatcher = "akka.actor.internal-dispatcher"
    pruning-interval = 120 s
    max-pruning-dissemination = 300 s
    pruning-marker-time-to-live = 6 h
    serializer-cache-time-to-live = 10s
    prefer-oldest = off

    delta-crdt {
      enabled = on
      max-delta-size = 50
    }

    durable {
      keys = []
      pruning-marker-time-to-live = 10 d
      store-actor-class = akka.cluster.ddata.LmdbDurableStore
      use-dispatcher = akka.cluster.distributed-data.durable.pinned-store

      pinned-store {
        executor = thread-pool-executor
        type = PinnedDispatcher
      }

      lmdb {
        dir = "ddata"
        map-size = 100 MiB
        write-behind-interval = off
      }
    }
  }
  scheduler {
    tick-duration = 10ms
    ticks-per-wheel = 512
    shutdown-timeout = 5s
    implementation = akka.actor.LightArrayRevolverScheduler
  }

  io {

      # By default the select loops run on dedicated threads, hence using a
      # PinnedDispatcher
      pinned-dispatcher {
        type = "PinnedDispatcher"
        executor = "thread-pool-executor"
        thread-pool-executor {
                # Keep alive time for threads
                keep-alive-time = 60s

                # Define a fixed thread pool size with this property. The corePoolSize
                # and the maximumPoolSize of the ThreadPoolExecutor will be set to this
                # value, if it is defined. Then the other pool-size properties will not
                # be used.
                #
                # Valid values are: `off` or a positive integer.
                fixed-pool-size = off

                # Min number of threads to cap factor-based corePoolSize number to
                core-pool-size-min = 8

                # The core-pool-size-factor is used to determine corePoolSize of the
                # ThreadPoolExecutor using the following formula:
                # ceil(available processors * factor).
                # Resulting size is then bounded by the core-pool-size-min and
                # core-pool-size-max values.
                core-pool-size-factor = 3.0

                # Max number of threads to cap factor-based corePoolSize number to
                core-pool-size-max = 64

                # Minimum number of threads to cap factor-based maximumPoolSize number to
                max-pool-size-min = 8

                # The max-pool-size-factor is used to determine maximumPoolSize of the
                # ThreadPoolExecutor using the following formula:
                # ceil(available processors * factor)
                # The maximumPoolSize will not be less than corePoolSize.
                # It is only used if using a bounded task queue.
                max-pool-size-factor  = 3.0

                # Max number of threads to cap factor-based maximumPoolSize number to
                max-pool-size-max = 64

                # Specifies the bounded capacity of the task queue (< 1 == unbounded)
                task-queue-size = -1

                # Specifies which type of task queue will be used, can be "array" or
                # "linked" (default)
                task-queue-type = "linked"

                # Allow core threads to time out
                allow-core-timeout = on
              }
      }

      tcp {

        # The number of selectors to stripe the served channels over; each of
        # these will use one select loop on the selector-dispatcher.
        nr-of-selectors = 1

        # Maximum number of open channels supported by this TCP module; there is
        # no intrinsic general limit, this setting is meant to enable DoS
        # protection by limiting the number of concurrently connected clients.
        # Also note that this is a "soft" limit; in certain cases the implementation
        # will accept a few connections more or a few less than the number configured
        # here. Must be an integer > 0 or "unlimited".
        max-channels = 256000

        # When trying to assign a new connection to a selector and the chosen
        # selector is at full capacity, retry selector choosing and assignment
        # this many times before giving up
        selector-association-retries = 10

        # The maximum number of connection that are accepted in one go,
        # higher numbers decrease latency, lower numbers increase fairness on
        # the worker-dispatcher
        batch-accept-limit = 10

        # The number of bytes per direct buffer in the pool used to read or write
        # network data from the kernel.
        direct-buffer-size = 128 KiB

        # The maximal number of direct buffers kept in the direct buffer pool for
        # reuse.
        direct-buffer-pool-limit = 1000

        # The duration a connection actor waits for a `Register` message from
        # its commander before aborting the connection.
        register-timeout = 5s

        # The maximum number of bytes delivered by a `Received` message. Before
        # more data is read from the network the connection actor will try to
        # do other work.
        # The purpose of this setting is to impose a smaller limit than the
        # configured receive buffer size. When using value 'unlimited' it will
        # try to read all from the receive buffer.
        max-received-message-size = unlimited

        # Enable fine grained logging of what goes on inside the implementation.
        # Be aware that this may log more than once per message sent to the actors
        # of the tcp implementation.
        trace-logging = off

        # Fully qualified config path which holds the dispatcher configuration
        # to be used for running the select() calls in the selectors
        selector-dispatcher = "akka.io.pinned-dispatcher"

        # Fully qualified config path which holds the dispatcher configuration
        # for the read/write worker actors
        worker-dispatcher = "akka.actor.internal-dispatcher"

        # Fully qualified config path which holds the dispatcher configuration
        # for the selector management actors
        management-dispatcher = "akka.actor.internal-dispatcher"

        # Fully qualified config path which holds the dispatcher configuration
        # on which file IO tasks are scheduled
        file-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"

        # The maximum number of bytes (or "unlimited") to transfer in one batch
        # when using `WriteFile` command which uses `FileChannel.transferTo` to
        # pipe files to a TCP socket. On some OS like Linux `FileChannel.transferTo`
        # may block for a long time when network IO is faster than file IO.
        # Decreasing the value may improve fairness while increasing may improve
        # throughput.
        file-io-transferTo-limit = 512 KiB

        # The number of times to retry the `finishConnect` call after being notified about
        # OP_CONNECT. Retries are needed if the OP_CONNECT notification doesn't imply that
        # `finishConnect` will succeed, which is the case on Android.
        finish-connect-retries = 5

        # On Windows connection aborts are not reliably detected unless an OP_READ is
        # registered on the selector _after_ the connection has been reset. This
        # workaround enables an OP_CONNECT which forces the abort to be visible on Windows.
        # Enabling this setting on other platforms than Windows will cause various failures
        # and undefined behavior.
        # Possible values of this key are on, off and auto where auto will enable the
        # workaround if Windows is detected automatically.
        windows-connection-abort-workaround-enabled = off
      }

      udp {

        # The number of selectors to stripe the served channels over; each of
        # these will use one select loop on the selector-dispatcher.
        nr-of-selectors = 1

        # Maximum number of open channels supported by this UDP module Generally
        # UDP does not require a large number of channels, therefore it is
        # recommended to keep this setting low.
        max-channels = 4096

        # The select loop can be used in two modes:
        # - setting "infinite" will select without a timeout, hogging a thread
        # - setting a positive timeout will do a bounded select call,
        #   enabling sharing of a single thread between multiple selectors
        #   (in this case you will have to use a different configuration for the
        #   selector-dispatcher, e.g. using "type=Dispatcher" with size 1)
        # - setting it to zero means polling, i.e. calling selectNow()
        select-timeout = infinite

        # When trying to assign a new connection to a selector and the chosen
        # selector is at full capacity, retry selector choosing and assignment
        # this many times before giving up
        selector-association-retries = 10

        # The maximum number of datagrams that are read in one go,
        # higher numbers decrease latency, lower numbers increase fairness on
        # the worker-dispatcher
        receive-throughput = 3

        # The number of bytes per direct buffer in the pool used to read or write
        # network data from the kernel.
        direct-buffer-size = 128 KiB

        # The maximal number of direct buffers kept in the direct buffer pool for
        # reuse.
        direct-buffer-pool-limit = 1000

        # Enable fine grained logging of what goes on inside the implementation.
        # Be aware that this may log more than once per message sent to the actors
        # of the tcp implementation.
        trace-logging = off

        # Fully qualified config path which holds the dispatcher configuration
        # to be used for running the select() calls in the selectors
        selector-dispatcher = "akka.io.pinned-dispatcher"

        # Fully qualified config path which holds the dispatcher configuration
        # for the read/write worker actors
        worker-dispatcher = "akka.actor.internal-dispatcher"

        # Fully qualified config path which holds the dispatcher configuration
        # for the selector management actors
        management-dispatcher = "akka.actor.internal-dispatcher"
      }

      udp-connected {

        # The number of selectors to stripe the served channels over; each of
        # these will use one select loop on the selector-dispatcher.
        nr-of-selectors = 1

        # Maximum number of open channels supported by this UDP module Generally
        # UDP does not require a large number of channels, therefore it is
        # recommended to keep this setting low.
        max-channels = 4096

        # The select loop can be used in two modes:
        # - setting "infinite" will select without a timeout, hogging a thread
        # - setting a positive timeout will do a bounded select call,
        #   enabling sharing of a single thread between multiple selectors
        #   (in this case you will have to use a different configuration for the
        #   selector-dispatcher, e.g. using "type=Dispatcher" with size 1)
        # - setting it to zero means polling, i.e. calling selectNow()
        select-timeout = infinite

        # When trying to assign a new connection to a selector and the chosen
        # selector is at full capacity, retry selector choosing and assignment
        # this many times before giving up
        selector-association-retries = 10

        # The maximum number of datagrams that are read in one go,
        # higher numbers decrease latency, lower numbers increase fairness on
        # the worker-dispatcher
        receive-throughput = 3

        # The number of bytes per direct buffer in the pool used to read or write
        # network data from the kernel.
        direct-buffer-size = 128 KiB

        # The maximal number of direct buffers kept in the direct buffer pool for
        # reuse.
        direct-buffer-pool-limit = 1000

        # Enable fine grained logging of what goes on inside the implementation.
        # Be aware that this may log more than once per message sent to the actors
        # of the tcp implementation.
        trace-logging = off

        # Fully qualified config path which holds the dispatcher configuration
        # to be used for running the select() calls in the selectors
        selector-dispatcher = "akka.io.pinned-dispatcher"

        # Fully qualified config path which holds the dispatcher configuration
        # for the read/write worker actors
        worker-dispatcher = "akka.actor.internal-dispatcher"

        # Fully qualified config path which holds the dispatcher configuration
        # for the selector management actors
        management-dispatcher = "akka.actor.internal-dispatcher"
      }

      dns {
        # Fully qualified config path which holds the dispatcher configuration
        # for the manager and resolver router actors.
        # For actual router configuration see akka.actor.deployment./IO-DNS/*
        dispatcher = "akka.actor.internal-dispatcher"

        # Name of the subconfig at path akka.io.dns, see inet-address below
        #
        # Change to `async-dns` to use the new "native" DNS resolver,
        # which is also capable of resolving SRV records.
        resolver = "inet-address"

        # To-be-deprecated DNS resolver implementation which uses the Java InetAddress to resolve DNS records.
        # To be replaced by `akka.io.dns.async` which implements the DNS protocol natively and without blocking (which InetAddress does)
        inet-address {
          # This configuration entry is deprecated since Akka 2.6.0 and functionality for defining a custom provider
          # will be removed in a future Akka version
          provider-object = "akka.io.InetAddressDnsProvider"

          # To set the time to cache name resolutions
          # Possible values:
          # default: sun.net.InetAddressCachePolicy.get() and getNegative()
          # forever: cache forever
          # never: no caching
          # n [time unit]: positive timeout with unit, for example 30s
          positive-ttl = default
          negative-ttl = default

          # How often to sweep out expired cache entries.
          # Note that this interval has nothing to do with TTLs
          cache-cleanup-interval = 120s
        }

        async-dns {
          # This configuration entry is deprecated since Akka 2.6.0 and functionality for defining a custom provider
          # will be removed in a future Akka version
          provider-object = "akka.io.dns.internal.AsyncDnsProvider"

          # Set upper bound for caching successfully resolved dns entries
          # if the DNS record has a smaller TTL value than the setting that
          # will be used. Default is to use the record TTL with no cap.
          # Possible values:
          # forever: always use the minimum TTL from the found records
          # never: never cache
          # n [time unit] = cap the caching to this value
          positive-ttl = forever

          # Set how long the fact that a DNS record could not be found is
          # cached. If a new resolution is done while the fact is cached it will
          # be failed and not result in an actual DNS resolution. Default is
          # to never cache.
          # Possible values:
          # never: never cache
          # forever: cache a missing DNS record forever (you probably will not want to do this)
          # n [time unit] = cache for this long
          negative-ttl = never

          # Configures nameservers to query during DNS resolution.
          # Defaults to the nameservers that would be used by the JVM by default.
          # Set to a list of IPs to override the servers, e.g. [ "8.8.8.8", "8.8.4.4" ] for Google's servers
          # If multiple are defined then they are tried in order until one responds
          nameservers = default

          # The time that a request is allowed to live before being discarded
          # given no reply. The lower bound of this should always be the amount
          # of time to reasonably expect a DNS server to reply within.
          # If multiple name servers are provided then each gets this long to response before trying
          # the next one
          resolve-timeout = 5s

          # How often to sweep out expired cache entries.
          # Note that this interval has nothing to do with TTLs
          cache-cleanup-interval = 120s

          # Configures the list of search domains.
          # Defaults to a system dependent lookup (on Unix like OSes, will attempt to parse /etc/resolv.conf, on
          # other platforms, will not make any attempt to lookup the search domains). Set to a single domain, or
          # a list of domains, eg, [ "example.com", "example.net" ].
          search-domains = default

          # Any hosts that have a number of dots less than this will not be looked up directly, instead, a search on
          # the search domains will be tried first. This corresponds to the ndots option in /etc/resolv.conf, see
          # https://linux.die.net/man/5/resolver for more info.
          # Defaults to a system dependent lookup (on Unix like OSes, will attempt to parse /etc/resolv.conf, on
          # other platforms, will default to 1).
          ndots = default

          # Which SecureRandom algorithm to use for generating DNS request IDs.  The default "" or "SecureRandom"
          # is likely sufficient, but you may supply an alternative algorithm, in which case resolution will
          # proceed as in `SecureRandom.getInstance()`
          id-strategy = ""
        }
      }
    }
}